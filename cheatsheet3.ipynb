{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8f9da-90f5-49cd-bc88-3c687c276156",
   "metadata": {},
   "source": [
    "# Cheat Sheet 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14f506-d442-437d-8499-a827f256d5e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d73e9-c495-47a3-9765-006efc87fffc",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "### Requests Module & The Entrez API\n",
    "In this first exercise, we're tasked with using the request module to find some papers on PubMed through the Entrez API. This process will require that we use several modules, so let's start by importing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b928de33-ec45-48da-8cc1-e0f0cb261014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import itertools\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aee485-908c-456a-9484-bfc1216a6341",
   "metadata": {},
   "source": [
    "Let's find and parse some data about covid-19 articles in this way. First we have to build our URL according to the Entrez API specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "deb5c94d-cf3b-4e69-be44-f500447cd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"stroke\"\n",
    "year = 2020\n",
    "retmax = 20\n",
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "parameters = f\"?db=pubmed&retmax={retmax}&term={search_term}+AND+{year}[pdat]\"\n",
    "url = base_url + parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb98aa-8d83-4cce-b725-15ef087de3fd",
   "metadata": {},
   "source": [
    "The base URL specificies the website and endpoint we would like to request our data from. The parameters allow us to tell this endpoint exactly what we're looking for, including what database we'd like to access (db=pubmed), how many articles we'd like to see (redmax=20), what and articles we'd like to search for (term=Coronavirus+AND+2019). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e8f37db-00e0-4581-a324-44490c0295c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d5801-d6a3-4ba8-8449-64680fcbcfed",
   "metadata": {},
   "source": [
    "The function requests.get() should return the server's response to your request.\n",
    "Let's have a look at what this response is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6746988-48d7-4dae-b847-4639eeecdac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa9909c0-c3e1-41df-a848-e84d3b35f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dir? help? attr?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d729c-8f39-40ff-b9d8-1cc63dc246bc",
   "metadata": {},
   "source": [
    "Printing r directly to the console gives us some vague description of a response object with code 200. This is an HTTP response status code, which tells us whether or not our request was succesful. The code 200 means \"OK\", which is a good sign that our request went through. You can find what other codes mean [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), just in case you get something else.\n",
    "\n",
    "Now that we have a response object, we can start extracting information. For example, we can get the status code from the object's properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c958bc35-be33-474b-ba66-3d8478d87c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0a37b-7965-4032-88df-ef996728224b",
   "metadata": {},
   "source": [
    "We can also get the content of the response using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f8e2000f-4333-44db-9ca2-e91fcd1c599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
      "<!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"https://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\">\n",
      "<eSearchResult><Count>30633</Count><RetMax>20</RetMax><RetStart>0</RetStart><IdList>\n",
      "<Id>35353466</Id>\n",
      "<Id>35342805</Id>\n",
      "<Id>35279226</Id>\n",
      "<Id>35130113</Id>\n",
      "<Id>35074187</Id>\n",
      "<Id>35002168</Id>\n",
      "<Id>34950386</Id>\n",
      "<Id>34950327</Id>\n",
      "<Id>34898801</Id>\n",
      "<Id>34888207</Id>\n",
      "<Id>34795974</Id>\n",
      "<Id>34765407</Id>\n",
      "<Id>34752535</Id>\n",
      "<Id>34732927</Id>\n",
      "<Id>34728943</Id>\n",
      "<Id>34720138</Id>\n",
      "<Id>34713244</Id>\n",
      "<Id>34713060</Id>\n",
      "<Id>34695217</Id>\n",
      "<Id>34654533</Id>\n",
      "</IdList><TranslationSet><Translation>     <From>stroke</From>     <To>\"stroke\"[MeSH Terms] OR \"stroke\"[All Fields]</To>    </Translation></TranslationSet><TranslationStack>   <TermSet>    <Term>\"stroke\"[MeSH Terms]</Term>    <Field>MeSH Terms</Field>    <Count>159133</Count>    <Explode>Y</Explode>   </TermSet>   <TermSet>    <Term>\"stroke\"[All Fields]</Term>    <Field>All Fields</Field>    <Count>373285</Count>    <Explode>N</Explode>   </TermSet>   <OP>OR</OP>   <OP>GROUP</OP>   <TermSet>    <Term>2020[pdat]</Term>    <Field>pdat</Field>    <Count>1631785</Count>    <Explode>N</Explode>   </TermSet>   <OP>AND</OP>  </TranslationStack><QueryTranslation>(\"stroke\"[MeSH Terms] OR \"stroke\"[All Fields]) AND 2020[pdat]</QueryTranslation></eSearchResult>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = r.text\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeb518-54ee-4f6b-8367-0d570472f0b2",
   "metadata": {},
   "source": [
    "The above printout statement clearly indicates that our data is formatted as an XML string. We can use the xml.etree.ElementTree module to parse this XML string into a DOM and extract our desired pubmed ID's (see Cheat Sheet 2, if you need a refresher on parsing XML):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c454a2ee-adfa-4876-be11-c044c06cf842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.ElementTree(ET.fromstring(content))\n",
    "root = tree.getroot()\n",
    "ids = [Id.text for Id in root.iter('Id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b80a1264-f4a6-481c-a0f8-87f7a1f36c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['35353466', '35342805', '35279226', '35130113', '35074187', '35002168', '34950386', '34950327', '34898801', '34888207', '34795974', '34765407', '34752535', '34732927', '34728943', '34720138', '34713244', '34713060', '34695217', '34654533']\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc932431-0053-44ac-ab00-a588d72726de",
   "metadata": {},
   "source": [
    "You may find it useful to adapt the code above into a function that returns some pubmed ID's based on search parameters. This will help in case you have to search for several different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70459bc3-9978-48bd-95f1-72b072ea5a9b",
   "metadata": {},
   "source": [
    "Now that we have our paper ID's, we will need to make another request to **a different endpoint** to get some metadata back. We can ask for the metadata from multiple papers at once by specifiying setting id search parameter to be a collection of pubmed ID's separated by commas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4e6d2-7cf4-4817-91dd-a8a21b9eab0a",
   "metadata": {},
   "source": [
    "**WARNING: The Entrez API only allows a limited length of URL, so you may have to request papers in small batches. If this is the case, you absolutely must space out the requests you send to the server using the time module, or else they will revoke your IP address's access to data. Below, I'm using the time module to spread out multiple runs through the same loop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ed927177-7745-4f0a-a693-004aa464683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    time.sleep(1)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83ee7b7c-8566-4798-ad13-ae9b9dbe9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_string = \",\".join(ids)# joins list of ids to comma separated string\n",
    "\n",
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "parameters =f\"?db=pubmed&retmode=xml&id={id_string}\"\n",
    "\n",
    "url = base_url + parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ba237-c701-4b6c-ad94-72a825f0bd94",
   "metadata": {},
   "source": [
    "That warning aside, let's get some data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91331f89-3f05-4501-bdb3-7fd796ac74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da7da84-8667-47ce-8576-670c1a28a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<!DOCTYPE PubmedArticleSet PUBLIC \"-//NLM//DTD PubMedArticle, 1st January 2019//EN\" \"https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_190101.dtd\">\n",
      "<PubmedArticleSet><PubmedArticle><MedlineCitation Status=\"MEDLINE\" IndexingMethod=\"Automated\" Owner=\"NLM\"><PMID Version=\"1\">35516164</PMID><DateCompleted><Year>2022</Year><Month>05</Month><Day>09</Day></DateCompleted><DateRevised><Year>2022</Year><Month>05</Month><Day>09</Day></DateRevised><Article PubModel=\"Electronic-eCollection\"><Journal><ISSN IssnType=\"Electronic\">2399-4908</ISSN><JournalIssue CitedMedium=\"Internet\"><Volume>5</Volume><Issue>4</Issue><PubDate><Year>2020</Year></PubDate></JournalIssue><Title>International journal of population data science</Title><ISOAbbreviation>Int J Popul Data Sci</ISOAbbreviation></Journal><ArticleTitle>Estimating surge in COVID-19 cases, hospital resources and PPE demand with the interactive and locally-informed <i>COVID-19 Health System Capacity Planning Tool</i>.</ArticleTitle><\n"
     ]
    }
   ],
   "source": [
    "print(r.text[0:1000]) #only prints out the first thousand characters of our metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80da3-007f-405d-9b61-aed33744a574",
   "metadata": {},
   "source": [
    "Again, our response text is just a string containing XML-formatted metadata. Since we have already gone over XML parsing, I will leave it to you to figure out the internal structure of the document and extract some information using the ElementTree module.\n",
    "\n",
    "**Hint**: for XML parsing for very large, complex, deeply nested XML responses such as this, you might find it useful to write your XML data to a file and look at it using an editor with syntax highlighting to get a broad overview of its structure. This is especially useful when you're trying to write code that pulls specific information out XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970128e8-bee7-4bb5-be6f-ab0ea2fd2ba9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23efa80-b69b-47af-91c4-9a7ea44091a5",
   "metadata": {},
   "source": [
    "### JavaScript Object Notation (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa114fc5-34ae-4c2c-af85-11587fd800b3",
   "metadata": {},
   "source": [
    "Like XML, JSON is a data format that is commonly used to pass around and store structured information. Like XML, JSON is flexible and capable of storing complex, nested information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96444ec-28bd-4289-922b-07e52494fd0f",
   "metadata": {},
   "source": [
    "Here's an example of JSON:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5a3de-d1f5-4910-b86c-90f45b8ae53b",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"name\": \"Voldemort\",\n",
    "    \"evil?\": true,\n",
    "    \"birth year\": 1926,\n",
    "    \"henchman\": [\n",
    "                     \"Lucius Malfoy\", \n",
    "                     \"Severus Snape\", \n",
    "                     \"Belatrix Lestrange\",\n",
    "                     \"Peter Pettigrew\"\n",
    "                ],\n",
    "    \"facial features\":  {\n",
    "                            \"nose\": null,\n",
    "                            \"skin\": \"ghastly\"\n",
    "                        }  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542dd76-2df3-4a29-9b9c-5334c53bdc8a",
   "metadata": {},
   "source": [
    "JSON has two basic structural components that allow us to organize this information: objects and arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4a04c-4c4a-414f-89e8-7c0f17401deb",
   "metadata": {},
   "source": [
    "**Objects** are very similar in structure to python dictionaries. Like dictionaries, objects store information using key-value pairs. While dictionary keys can be any immutable type, objects can only use strings as keys. Object values, on the other hand, can be strings, numbers, booleans, nulls, arrays, and even other objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c0b7d-0745-4b53-b77b-8006b87fddb0",
   "metadata": {},
   "source": [
    "The syntax of an object is quite simple, and is described by \n",
    "1. Objects are delimited by curly brackets **{  }**. \n",
    "\n",
    "2. An individual key-value pair is stored as **\"key\": value**, with a colon separating the key and value.\n",
    "\n",
    "3. Key-value pairs are separated from each other by commas\n",
    "\n",
    "3. Whitespace and new lines do not affect the structure, so we can use them as we see fit for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889f227-3a4d-41f5-a03d-975c650d5932",
   "metadata": {},
   "source": [
    "Putting these rules together, here's what a basic object looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c9885-ad17-450c-ba11-24762eaf4e3a",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"key1\": \"value1\",\n",
    "    \"key2\": 2,\n",
    "    \"key3\": true,\n",
    "    \"key4\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336c6a2-e501-4d18-9485-dddcdfb0a47b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e234ab-6b5f-4aa1-8832-7795c23eb7d9",
   "metadata": {},
   "source": [
    "**Arrays** are very similar in structure to python lists. Like lists, they store an ordered collection of data across a range of integer indices. We often find that all the members of a given array are of the same type, but this is not always the case. Arrays can store strings, numbers, booleans, nulls, objects, and other arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d4060-3d0b-4236-93ca-5dafd6d950d2",
   "metadata": {},
   "source": [
    "Array syntax is also very simple:\n",
    "1. Arrays are delimited by square brackets **[   ]**\n",
    "\n",
    "2. Elements inside an array are separated by commas\n",
    "\n",
    "3. Whitespace and new lines do not affect the structure, so we can use them as we see fit for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e066c15-334d-4bc4-9cab-cb7e7f52edaf",
   "metadata": {},
   "source": [
    "Putting these rules together, here's a basic JSON array:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd8ebc-809c-4661-a718-52be8c6e2aec",
   "metadata": {},
   "source": [
    "```json\n",
    "    [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcf9cf-e30f-4e6b-a95c-1656fe4c4014",
   "metadata": {},
   "source": [
    "A few more notes on JSON syntax:\n",
    "1. Short objects and arrays can be written inline like so:\n",
    "    - ```json \n",
    "        {\"name\": \"tom\", \"age\": 24}\n",
    "      ```\n",
    "    - ```json \n",
    "        [1, 2, 3]\n",
    "      ```\n",
    "2. For readability, larger objects and arrays should use line separation and tabbing:\n",
    "    - ```json \n",
    "        {\n",
    "            \"name\": \"tom\", \n",
    "            \"age\": 24,\n",
    "            \"hobbies\": [\"reading\",\n",
    "                        \"long walks on the beach\",\n",
    "                        \"cooking\",\n",
    "                        \"pickleball\"]\n",
    "         }\n",
    "      ```\n",
    "3. JSON strings must be delimited with double quotes, whereas python strings can use either double or single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c066e83-a9d7-4ad1-9626-c20ec33b06be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240ea27-846c-46bc-829f-87ce4b6721d0",
   "metadata": {},
   "source": [
    "Now that we understand what JSON is, how do we use it in Python? We'll start by importing python's internal json module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "274e0ff5-50f5-402c-84f8-d48f7a4c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7f4dd-002d-499e-9f0d-aeaaf8d47592",
   "metadata": {},
   "source": [
    "The json module automatically converts JSON structures to their Python equivalents and vice versa. Below is a conversion table that describes these data type equivalences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f6ef2-0d01-41e2-9511-c409e164faf8",
   "metadata": {},
   "source": [
    "|JSON | Python|\n",
    "|-----|-------|\n",
    "|object <br> {\"hello\": \"world\"}|dictionary <br> {'hello': \"world\"}|\n",
    "|array <br> [1,2,3] |list <br> [1,2,3]|\n",
    "|string<br>\"mystring\" | string <br><ul><li>\"mystring\"</li><li>'mystring'</li></ul>|\n",
    "|number<br> 5| int/long <br> 5|\n",
    "|number<br> 3.14 | float <br> 3.14|\n",
    "|Boolean<br><ul><li>true</li><li>false</li></ul> | bool <br><ul><li>True</li><li>False</li></ul>|\n",
    "|null| None|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c431e-5875-42b5-89e2-7385407c1a91",
   "metadata": {},
   "source": [
    "JSON is often stored and passed as plain text. Here we will use the json module to parse the plain text JSON object at the top of the section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6bbaff0-ed49-4490-837b-9693c468e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth year': 1926,\n",
      " 'evil?': True,\n",
      " 'facial features': {'nose': None, 'skin': 'ghastly'},\n",
      " 'henchman': ['Lucius Malfoy',\n",
      "              'Severus Snape',\n",
      "              'Belatrix Lestrange',\n",
      "              'Peter Pettigrew'],\n",
      " 'name': 'Voldemort'}\n"
     ]
    }
   ],
   "source": [
    "#block quotes like this allow us to see the indenting and line breaks inside a string.\n",
    "json_text = '''{\n",
    "    \"name\": \"Voldemort\",\n",
    "    \"evil?\": true,\n",
    "    \"birth year\": 1926,\n",
    "    \"henchman\": [\n",
    "                     \"Lucius Malfoy\", \n",
    "                     \"Severus Snape\", \n",
    "                     \"Belatrix Lestrange\",\n",
    "                     \"Peter Pettigrew\"\n",
    "                ],\n",
    "    \"facial features\":  {\n",
    "                            \"nose\": null,\n",
    "                            \"skin\": \"ghastly\"\n",
    "                        }  \n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "json_dict = json.loads(json_text)\n",
    "pp(json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7b4b2-0a39-485c-bcb8-e967974725c6",
   "metadata": {},
   "source": [
    "As we can see, the json.loads() function will parse a string containing JSON into a nested structure of python dictionaries and lists. To go in the other direction, we use the json.dumps() function. This direction requires an extra choice from the programmer: how do we want to format the text? There's not necessarily one right answer, just pick formatting parameters that make sense for you or your team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87cc89c7-5a99-4768-b020-57ce16bbf4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"birth year\": 1926,\n",
      "    \"evil?\": true,\n",
      "    \"facial features\": {\n",
      "        \"nose\": null,\n",
      "        \"skin\": \"ghastly\"\n",
      "    },\n",
      "    \"henchman\": [\n",
      "        \"Lucius Malfoy\",\n",
      "        \"Severus Snape\",\n",
      "        \"Belatrix Lestrange\",\n",
      "        \"Peter Pettigrew\"\n",
      "    ],\n",
      "    \"name\": \"Voldemort\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "new_text = json.dumps(json_dict, indent=4, sort_keys=True)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9554fe2-9946-4c69-97b4-cbbaa935d01b",
   "metadata": {},
   "source": [
    "In case you have collected, processed, and formatted large quantities of data from an API, you should probably save that data for later use. This way, we don't have to bother the API for the same data more than once (very important), and you don't have to process it again. As an added bonus, this makes our data pipeline more **modular**, which helps us break the problem up into separate steps. We can write JSON to a file by using the json.dump() function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b9c3bf4a-2c1e-40e8-9502-5278a859d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./voldemort.json\", \"w\") as json_file:\n",
    "    json.dump(json_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4563b6-7a86-4618-a2de-57bfb6dec439",
   "metadata": {},
   "source": [
    "Check to see that \"voldemort.json\" has been added to the folder containing this file.\n",
    "\n",
    "When we're ready to come back and use our data, we can read the file using the json.load() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8dd8d609-167e-4f97-bfe5-f153cdd8c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth year': 1926,\n",
      " 'evil?': True,\n",
      " 'facial features': {'nose': None, 'skin': 'ghastly'},\n",
      " 'henchman': ['Lucius Malfoy',\n",
      "              'Severus Snape',\n",
      "              'Belatrix Lestrange',\n",
      "              'Peter Pettigrew'],\n",
      " 'name': 'Voldemort'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./voldemort.json\", \"r\") as json_file:\n",
    "    dict_from_file = json.load(json_file)\n",
    "    pp(dict_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3aea3-dde6-4376-88f8-5c50c2322a75",
   "metadata": {},
   "source": [
    "WARNING: it is easy confuse the purpose of json.load() versus json.loads() and json.dump() versus json.dumps(). I find it helpful to pretend that the \"S\" in \"loads\" and \"dumps\" stands for \"String\", so these functions convert directly between **string** objects and dictionaries. By process of elimination, we can conclude that load and dump do not convert directly to strings, so they must interact with files instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8602e-ebea-4007-8a01-4d6f540db808",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8ee4a-138d-4279-91c1-4a29c0c87685",
   "metadata": {},
   "source": [
    "By now, we have gone over most of the tools for you to complete this problem on your own. If you get stuck, refer back to the section on basic python and pandas from cheatsheet1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83703155-0660-46f1-b855-2543f1cd953c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991c3c9-1bc8-4a6f-b3cf-be9c98723fe0",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f228a-6686-45ec-a520-3b41a982c136",
   "metadata": {},
   "source": [
    "Now we are going to get into the nitty gritty of machine learning. As you may know, machine learning is broadly used to describe algorithms that iteratively improve some model using data. As the model consumes more data, it tends to become more accurate, so we say that the machine \"learns\". This technique can take many different forms and create models for virtually any imaginable purpose. Roughly speaking, there are two coarse categories that we can separate machine learning algorithms into: supervised and unsupervised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820154e5-57e2-485b-9158-35fc6fafcaef",
   "metadata": {},
   "source": [
    "**A supervised learning algorithm** learns by training on labeled data, which is comprised of many input-output pairs. At each iteration, the model takes an input and makes a prediciton on what the output should be. The learning algorithm then compares this predicted output with the actual output that was paired with the input in the label data set. Based on this comparison, the learning algorithm adjusts the model's internal parameters slightly in whatever direction would best improve the prediction to be closer to the actual output. By iterating over this process many times, the model will gradually have more and more accurate predictions. Thus supervised learning tends to be used to generate predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cb9de-64e0-40bf-a0c6-526aa8d7340e",
   "metadata": {},
   "source": [
    "**An unsupervised learning** learns by training on unlabeled data. Generally, these models attempt to determine some salient property of the dataset. The type of property we want to extract, and the algorithm we choose to extract it will largely depend on our use case. For example, if we want to discover naturally occuring clusters in our data, we might use the k-means algorithm. If we want to simplify a dataset with many variables, we might instead choose truncated singular value decomposition (SVD) to pick out features that best explain variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520548a-8407-408c-8a18-dfbae003f047",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba7c76-cc7e-4b18-8586-3e0d01a35b0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb546c0c-a498-40dd-ade4-9b28f52d8bf2",
   "metadata": {},
   "source": [
    "**Transformers** are a special type of artificial neural network that process sequential data such as text, images, or video. Like all neural networks, transformers are made up of layers of **neurons**, which are linked up by weighted connections. Also like all neural networks, transformers are primarily trained using a supervised learning algorithm called **backpropogation**, which iteratively tunes the weights of each connection starting from the output layer and working backwards to the input layer. In particular, transformers have an internal mechanism called self-attention that allow them to decide which parts of the data are most important, and give those parts of the data more weight in determining the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4036ede-3d80-4822-9389-82fb2768be6b",
   "metadata": {},
   "source": [
    "Transformers are often used to embed human-readable text into high dimensional vectors, which incredibly encode semantic information into geometric and algebraic relationships. For example, in the embedding given by the word2vec transformer, we might find that \"King\" - \"Man\" + \"Woman\" = \"Queen\", or that \"Actor\" - \"Talent\" + \"Ego\" = \"Jay Leto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb4b6f-afda-4e95-9c2a-a0243ce0e5c5",
   "metadata": {},
   "source": [
    "![](embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41c4963-a9fe-4679-be2c-ad5735aa8e54",
   "metadata": {},
   "source": [
    "In natural language processing, we will often use a transformer to map text to vectors as the first step in a larger model-training pipeline. In this exercise, you are tasked with using a pretrained transformer, SPECTER, to embed metadata text before using that embedding to train two other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38cb7f-370d-434c-84ed-727e7c27626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e8bee-3108-4b1a-9af4-8bba3f2bb9ba",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43276737-ffa4-45e8-9af5-9509e8fcc273",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd56315-3950-45c6-907a-4083ee73f697",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d54467-e79c-407b-afbe-a4fbb7ee8394",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529cac52-2349-4f34-93ab-e07774d317ec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13046d33-6691-4cae-a710-a072f1729fdf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaa834-3e46-4a59-a1c3-6cae9d1ce377",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedb264-dfa3-4933-be0e-13c18d2ec523",
   "metadata": {},
   "source": [
    "Here you are tasked with describing how to parallelize the merge sort algorithm using two processes. Before you try to parallelize it, let's try to get a good understanding of how the merge sort works in general. Suppose you would like to sort the following list of values into assending order:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c1dcd-0365-4886-ab81-8b397dc77349",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7, 6, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f2cf4-1da9-4410-b39a-08d50afac98b",
   "metadata": {},
   "source": [
    "We start by splitting the list into two equal halfs, effectively by making a copy of each half:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a03f79-2d48-41e7-8614-5279cd8de8db",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7, 6, 1]\n",
    "```\n",
    "$\\hspace{3.5em}\\swarrow\\searrow$\n",
    "```json\n",
    "[[5,7],[6,1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a165db1-1e34-4741-8d8b-f802bb41e58c",
   "metadata": {},
   "source": [
    "We continue to recursively break up each sub-list one at a time until we encounter two lists containing at most one element each. Breaking up the first sublist above, we arrive at the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc26c-bf02-4bb2-8ed9-b84ff520b2e2",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7]\n",
    "```\n",
    "\n",
    "$\\hspace{2em}\\swarrow\\searrow$\n",
    "\n",
    "```json\n",
    "[[5],[7]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a31a2-b45d-43c3-a93f-8c3184f3b989",
   "metadata": {},
   "source": [
    "Now that we have two sublists of size 1, it is time to merge. We accomplish this by comparing the first element from each sublist. Whichever is smaller goes is removed from the split sublist and appended to the end of the sublist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde7452-c7c6-4d09-a470-d70464818ccc",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5],[7]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 5 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[7]],\n",
    "    merged: [5]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d52dd-d09b-4a49-a491-d33465533b6c",
   "metadata": {},
   "source": [
    "Now we try to compare the first element of each of the split sublists. Since the first sublist has run out of elements, we have to put the rest of them in the merged sublist. We've already compared the smallest element of the second sublist with all of the elements of the merged list, so we can conclude that the the second sublist can get added to the end of the merged list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7313a3f-5a6b-4860-95ee-48e1b3e823ad",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[],[7]],\n",
    "    merged: [5]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of our second sublist are appended to the merged list\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [5,7]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2cecd-931f-44ab-ad43-3b46410a8efd",
   "metadata": {},
   "source": [
    "Now that the first pair of single-element sublists are merged, we rinse and repeat with the next sublist:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37035dc-5944-47c8-a2c1-c08cc63a970d",
   "metadata": {},
   "source": [
    "### Split\n",
    "```json\n",
    "[6, 1]\n",
    "```\n",
    "\n",
    "$\\hspace{2em}\\swarrow\\searrow$\n",
    "\n",
    "```json\n",
    "[[6],[1]]\n",
    "```\n",
    "\n",
    "### Merge\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[6],[1]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 1 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[6],[]],\n",
    "    merged: [1]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of our first sublist are appended to the merged list\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [1,6]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33f604-0032-4653-96f9-7cb3327b5f7b",
   "metadata": {},
   "source": [
    "Now all of the single element sublists have been merged into sorted lists. We start merging these larger lists, which is only slightly more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c015283-6407-4483-9b31-d645dde8366d",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[1,6]],\n",
    "    merged: []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8be5f5-202e-4a83-b7f7-4c79e6d73aa0",
   "metadata": {},
   "source": [
    "We compare the first element of each split sublist. Whichever is smaller goes in the merged sublist first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f7b-ec89-4338-a09b-e934bf88213c",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[1,6]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 1 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[5,7],[6]],\n",
    "    merged: [1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1761806-9509-4bcb-a3f9-0ca1362c21e7",
   "metadata": {},
   "source": [
    "Now the second split sublist has a new smallest element, so we have to make the comparison again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68291c-d586-4374-bee9-e2df708c2381",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[6]],\n",
    "    merged: [1]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 5 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[7],[6]],\n",
    "    merged: [1,5]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a11012-0bcd-44aa-a271-c244e4cc2a57",
   "metadata": {},
   "source": [
    "Repeating this step one more time, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312ca3b-cb67-4d28-b367-d92ac91ba786",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[7],[6]],\n",
    "    merged: [1,5]\n",
    "}\n",
    "```\n",
    "$\\hspace{4em}\\Downarrow$ 6 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[7],[]],\n",
    "    merged: [1,5,6]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a134d-e7a6-4cb2-b852-4f3206116bdc",
   "metadata": {},
   "source": [
    "Again, one of our split sublists has run out of elements, so we have to merge the rest of the other sublist into the merged list. Since the smallest element of this split sublist has already been compared with the largest element of the merged sublist, every element of the split sublist is larger than every element of the merged sublist. Hence, we just append all the elements from the split sublist to the end of the merged sublist:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b769d2-48bd-40f4-91bf-54e72881016e",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[7],[]],\n",
    "    merged: [1,5,6]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of split sublist appended to the end of merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [1,5,6,7]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908f70b-ce45-4653-b530-6fdefadeb6d2",
   "metadata": {},
   "source": [
    "And so our list is sorted, and our task is done. Note that the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4615ca-5c7e-4c2a-86db-6263567e9b4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73c0c327-3653-44cf-b277-c8793c1282f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60b74f25-659a-4c56-8267-6c0c1cd785c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201b12f9-36a6-445b-8f73-4a0d53ec6112",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79247bd4-3aef-4456-aa6b-e6b69df5dab2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b228d02e-3f65-4e45-9ed7-34af88673c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
