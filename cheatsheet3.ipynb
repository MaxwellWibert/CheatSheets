{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8f9da-90f5-49cd-bc88-3c687c276156",
   "metadata": {},
   "source": [
    "# Cheat Sheet 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e14f506-d442-437d-8499-a827f256d5e1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d73e9-c495-47a3-9765-006efc87fffc",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "### Requests Module & The Entrez API\n",
    "In this first exercise, we're tasked with using the request module to find some papers on PubMed through the Entrez API. This process will require that we use several modules, so let's start by importing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b928de33-ec45-48da-8cc1-e0f0cb261014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import itertools\n",
    "from pprint import pprint as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aee485-908c-456a-9484-bfc1216a6341",
   "metadata": {},
   "source": [
    "Let's find and parse some data about covid-19 articles in this way. First we have to build our URL according to the Entrez API specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb5c94d-cf3b-4e69-be44-f500447cd270",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = \"stroke\"\n",
    "year = 2020\n",
    "retmax = 20\n",
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "parameters = f\"?db=pubmed&retmax={retmax}&term={search_term}+AND+{year}[pdat]\"\n",
    "url = base_url + parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb98aa-8d83-4cce-b725-15ef087de3fd",
   "metadata": {},
   "source": [
    "The base URL specificies the website and endpoint we would like to request our data from. The parameters allow us to tell this endpoint exactly what we're looking for, including what database we'd like to access (db=pubmed), how many articles we'd like to see (redmax=20), what and articles we'd like to search for (term=Coronavirus+AND+2019). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8f37db-00e0-4581-a324-44490c0295c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d5801-d6a3-4ba8-8449-64680fcbcfed",
   "metadata": {},
   "source": [
    "The function requests.get() should return the server's response to your request.\n",
    "Let's have a look at what this response is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6746988-48d7-4dae-b847-4639eeecdac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9909c0-c3e1-41df-a848-e84d3b35f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add dir? help? attr?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85d729c-8f39-40ff-b9d8-1cc63dc246bc",
   "metadata": {},
   "source": [
    "Printing r directly to the console gives us some vague description of a response object with code 200. This is an HTTP response status code, which tells us whether or not our request was succesful. The code 200 means \"OK\", which is a good sign that our request went through. You can find what other codes mean [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), just in case you get something else.\n",
    "\n",
    "Now that we have a response object, we can start extracting information. For example, we can get the status code from the object's properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c958bc35-be33-474b-ba66-3d8478d87c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c0a37b-7965-4032-88df-ef996728224b",
   "metadata": {},
   "source": [
    "We can also get the content of the response using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e2000f-4333-44db-9ca2-e91fcd1c599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n",
      "<!DOCTYPE eSearchResult PUBLIC \"-//NLM//DTD esearch 20060628//EN\" \"https://eutils.ncbi.nlm.nih.gov/eutils/dtd/20060628/esearch.dtd\">\n",
      "<eSearchResult><Count>30647</Count><RetMax>20</RetMax><RetStart>0</RetStart><IdList>\n",
      "<Id>35615236</Id>\n",
      "<Id>35353466</Id>\n",
      "<Id>35342805</Id>\n",
      "<Id>35279226</Id>\n",
      "<Id>35130113</Id>\n",
      "<Id>35074187</Id>\n",
      "<Id>35002168</Id>\n",
      "<Id>34950386</Id>\n",
      "<Id>34950327</Id>\n",
      "<Id>34898801</Id>\n",
      "<Id>34888207</Id>\n",
      "<Id>34795974</Id>\n",
      "<Id>34765407</Id>\n",
      "<Id>34752535</Id>\n",
      "<Id>34732927</Id>\n",
      "<Id>34728943</Id>\n",
      "<Id>34720138</Id>\n",
      "<Id>34713244</Id>\n",
      "<Id>34713060</Id>\n",
      "<Id>34695217</Id>\n",
      "</IdList><TranslationSet><Translation>     <From>stroke</From>     <To>\"stroke\"[MeSH Terms] OR \"stroke\"[All Fields]</To>    </Translation></TranslationSet><TranslationStack>   <TermSet>    <Term>\"stroke\"[MeSH Terms]</Term>    <Field>MeSH Terms</Field>    <Count>159896</Count>    <Explode>Y</Explode>   </TermSet>   <TermSet>    <Term>\"stroke\"[All Fields]</Term>    <Field>All Fields</Field>    <Count>374882</Count>    <Explode>N</Explode>   </TermSet>   <OP>OR</OP>   <OP>GROUP</OP>   <TermSet>    <Term>2020[pdat]</Term>    <Field>pdat</Field>    <Count>1632080</Count>    <Explode>N</Explode>   </TermSet>   <OP>AND</OP>  </TranslationStack><QueryTranslation>(\"stroke\"[MeSH Terms] OR \"stroke\"[All Fields]) AND 2020[pdat]</QueryTranslation></eSearchResult>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content = r.text\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfeb518-54ee-4f6b-8367-0d570472f0b2",
   "metadata": {},
   "source": [
    "The above printout statement clearly indicates that our data is formatted as an XML string. We can use the xml.etree.ElementTree module to parse this XML string into a DOM and extract our desired pubmed ID's (see Cheat Sheet 2, if you need a refresher on parsing XML):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c454a2ee-adfa-4876-be11-c044c06cf842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.ElementTree(ET.fromstring(content))\n",
    "root = tree.getroot()\n",
    "ids = [Id.text for Id in root.iter('Id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b80a1264-f4a6-481c-a0f8-87f7a1f36c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['35615236', '35353466', '35342805', '35279226', '35130113', '35074187', '35002168', '34950386', '34950327', '34898801', '34888207', '34795974', '34765407', '34752535', '34732927', '34728943', '34720138', '34713244', '34713060', '34695217']\n"
     ]
    }
   ],
   "source": [
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc932431-0053-44ac-ab00-a588d72726de",
   "metadata": {},
   "source": [
    "You may find it useful to adapt the code above into a function that returns some pubmed ID's based on search parameters. This will help in case you have to search for several different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70459bc3-9978-48bd-95f1-72b072ea5a9b",
   "metadata": {},
   "source": [
    "Now that we have our paper ID's, we will need to make another request to **a different endpoint** to get some metadata back. We can ask for the metadata from multiple papers at once by specifiying setting id search parameter to be a collection of pubmed ID's separated by commas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4e6d2-7cf4-4817-91dd-a8a21b9eab0a",
   "metadata": {},
   "source": [
    "**WARNING: The Entrez API only allows a limited length of URL, so you may have to request papers in small batches. If this is the case, you absolutely must space out the requests you send to the server using the time module, or else they will revoke your IP address's access to data. Below, I'm using the time module to spread out multiple runs through the same loop:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed927177-7745-4f0a-a693-004aa464683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    time.sleep(1)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ee7b7c-8566-4798-ad13-ae9b9dbe9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_string = \",\".join(ids)# joins list of ids to comma separated string\n",
    "\n",
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "parameters =f\"?db=pubmed&retmode=xml&id={id_string}\"\n",
    "\n",
    "url = base_url + parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ba237-c701-4b6c-ad94-72a825f0bd94",
   "metadata": {},
   "source": [
    "That warning aside, let's get some data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91331f89-3f05-4501-bdb3-7fd796ac74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da7da84-8667-47ce-8576-670c1a28a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" ?>\n",
      "<!DOCTYPE PubmedArticleSet PUBLIC \"-//NLM//DTD PubMedArticle, 1st January 2019//EN\" \"https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_190101.dtd\">\n",
      "<PubmedArticleSet><PubmedArticle><MedlineCitation Status=\"PubMed-not-MEDLINE\" Owner=\"NLM\"><PMID Version=\"1\">35615236</PMID><DateRevised><Year>2022</Year><Month>05</Month><Day>26</Day></DateRevised><Article PubModel=\"Print-Electronic\"><Journal><ISSN IssnType=\"Print\">1751-1437</ISSN><JournalIssue CitedMedium=\"Print\"><Volume>23</Volume><Issue>2</Issue><PubDate><Year>2022</Year><Month>May</Month></PubDate></JournalIssue><Title>Journal of the Intensive Care Society</Title><ISOAbbreviation>J Intensive Care Soc</ISOAbbreviation></Journal><ArticleTitle>Cerebral oximetry in adult cardiac surgery to reduce the incidence of neurological impairment and hospital length-of-stay: A prospective, randomized, controlled trial.</ArticleTitle><Pagination><MedlinePgn>109-116</MedlinePgn></Pagination><ELocationID EIdType=\"doi\" ValidYN=\"Y\">10.\n"
     ]
    }
   ],
   "source": [
    "print(r.text[0:1000]) #only prints out the first thousand characters of our metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80da3-007f-405d-9b61-aed33744a574",
   "metadata": {},
   "source": [
    "Again, our response text is just a string containing XML-formatted metadata. Since we have already gone over XML parsing, I will leave it to you to figure out the internal structure of the document and extract some information using the ElementTree module.\n",
    "\n",
    "**Hint**: for XML parsing for very large, complex, deeply nested XML responses such as this, you might find it useful to write your XML data to a file and look at it using an editor with syntax highlighting to get a broad overview of its structure. This is especially useful when you're trying to write code that pulls specific information out XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970128e8-bee7-4bb5-be6f-ab0ea2fd2ba9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23efa80-b69b-47af-91c4-9a7ea44091a5",
   "metadata": {},
   "source": [
    "### JavaScript Object Notation (JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa114fc5-34ae-4c2c-af85-11587fd800b3",
   "metadata": {},
   "source": [
    "Like XML, JSON is a data format that is commonly used to pass around and store structured information. Like XML, JSON is flexible and capable of storing complex, nested information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96444ec-28bd-4289-922b-07e52494fd0f",
   "metadata": {},
   "source": [
    "Here's an example of JSON:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5a3de-d1f5-4910-b86c-90f45b8ae53b",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"name\": \"Voldemort\",\n",
    "    \"evil?\": true,\n",
    "    \"birth year\": 1926,\n",
    "    \"henchman\": [\n",
    "                     \"Lucius Malfoy\", \n",
    "                     \"Severus Snape\", \n",
    "                     \"Belatrix Lestrange\",\n",
    "                     \"Peter Pettigrew\"\n",
    "                ],\n",
    "    \"facial features\":  {\n",
    "                            \"nose\": null,\n",
    "                            \"skin\": \"ghastly\"\n",
    "                        }  \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542dd76-2df3-4a29-9b9c-5334c53bdc8a",
   "metadata": {},
   "source": [
    "JSON has two basic structural components that allow us to organize this information: objects and arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b4a04c-4c4a-414f-89e8-7c0f17401deb",
   "metadata": {},
   "source": [
    "**Objects** are very similar in structure to python dictionaries. Like dictionaries, objects store information using key-value pairs. While dictionary keys can be any immutable type, objects can only use strings as keys. Object values, on the other hand, can be strings, numbers, booleans, nulls, arrays, and even other objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c0b7d-0745-4b53-b77b-8006b87fddb0",
   "metadata": {},
   "source": [
    "The syntax of an object is quite simple, and is described by \n",
    "1. Objects are delimited by curly brackets **{  }**. \n",
    "\n",
    "2. An individual key-value pair is stored as **\"key\": value**, with a colon separating the key and value.\n",
    "\n",
    "3. Key-value pairs are separated from each other by commas\n",
    "\n",
    "3. Whitespace and new lines do not affect the structure, so we can use them as we see fit for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889f227-3a4d-41f5-a03d-975c650d5932",
   "metadata": {},
   "source": [
    "Putting these rules together, here's what a basic object looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c9885-ad17-450c-ba11-24762eaf4e3a",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"key1\": \"value1\",\n",
    "    \"key2\": 2,\n",
    "    \"key3\": true,\n",
    "    \"key4\": null\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336c6a2-e501-4d18-9485-dddcdfb0a47b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e234ab-6b5f-4aa1-8832-7795c23eb7d9",
   "metadata": {},
   "source": [
    "**Arrays** are very similar in structure to python lists. Like lists, they store an ordered collection of data across a range of integer indices. We often find that all the members of a given array are of the same type, but this is not always the case. Arrays can store strings, numbers, booleans, nulls, objects, and other arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1d4060-3d0b-4236-93ca-5dafd6d950d2",
   "metadata": {},
   "source": [
    "Array syntax is also very simple:\n",
    "1. Arrays are delimited by square brackets **[   ]**\n",
    "\n",
    "2. Elements inside an array are separated by commas\n",
    "\n",
    "3. Whitespace and new lines do not affect the structure, so we can use them as we see fit for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e066c15-334d-4bc4-9cab-cb7e7f52edaf",
   "metadata": {},
   "source": [
    "Putting these rules together, here's a basic JSON array:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd8ebc-809c-4661-a718-52be8c6e2aec",
   "metadata": {},
   "source": [
    "```json\n",
    "    [\n",
    "        1,\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dcf9cf-e30f-4e6b-a95c-1656fe4c4014",
   "metadata": {},
   "source": [
    "A few more notes on JSON syntax:\n",
    "1. Short objects and arrays can be written inline like so:\n",
    "    - ```json \n",
    "        {\"name\": \"tom\", \"age\": 24}\n",
    "      ```\n",
    "    - ```json \n",
    "        [1, 2, 3]\n",
    "      ```\n",
    "2. For readability, larger objects and arrays should use line separation and tabbing:\n",
    "    - ```json \n",
    "        {\n",
    "            \"name\": \"tom\", \n",
    "            \"age\": 24,\n",
    "            \"hobbies\": [\"reading\",\n",
    "                        \"long walks on the beach\",\n",
    "                        \"cooking\",\n",
    "                        \"pickleball\"]\n",
    "         }\n",
    "      ```\n",
    "3. JSON strings must be delimited with double quotes, whereas python strings can use either double or single quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c066e83-a9d7-4ad1-9626-c20ec33b06be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240ea27-846c-46bc-829f-87ce4b6721d0",
   "metadata": {},
   "source": [
    "Now that we understand what JSON is, how do we use it in Python? We'll start by importing python's internal json module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "274e0ff5-50f5-402c-84f8-d48f7a4c323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7f4dd-002d-499e-9f0d-aeaaf8d47592",
   "metadata": {},
   "source": [
    "The json module automatically converts JSON structures to their Python equivalents and vice versa. Below is a conversion table that describes these data type equivalences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f6ef2-0d01-41e2-9511-c409e164faf8",
   "metadata": {},
   "source": [
    "|JSON | Python|\n",
    "|-----|-------|\n",
    "|object <br> {\"hello\": \"world\"}|dictionary <br> {'hello': \"world\"}|\n",
    "|array <br> [1,2,3] |list <br> [1,2,3]|\n",
    "|string<br>\"mystring\" | string <br><ul><li>\"mystring\"</li><li>'mystring'</li></ul>|\n",
    "|number<br> 5| int/long <br> 5|\n",
    "|number<br> 3.14 | float <br> 3.14|\n",
    "|Boolean<br><ul><li>true</li><li>false</li></ul> | bool <br><ul><li>True</li><li>False</li></ul>|\n",
    "|null| None|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c431e-5875-42b5-89e2-7385407c1a91",
   "metadata": {},
   "source": [
    "JSON is often stored and passed as plain text. Here we will use the json module to parse the plain text JSON object at the top of the section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6bbaff0-ed49-4490-837b-9693c468e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth year': 1926,\n",
      " 'evil?': True,\n",
      " 'facial features': {'nose': None, 'skin': 'ghastly'},\n",
      " 'henchman': ['Lucius Malfoy',\n",
      "              'Severus Snape',\n",
      "              'Belatrix Lestrange',\n",
      "              'Peter Pettigrew'],\n",
      " 'name': 'Voldemort'}\n"
     ]
    }
   ],
   "source": [
    "#block quotes like this allow us to see the indenting and line breaks inside a string.\n",
    "json_text = '''{\n",
    "    \"name\": \"Voldemort\",\n",
    "    \"evil?\": true,\n",
    "    \"birth year\": 1926,\n",
    "    \"henchman\": [\n",
    "                     \"Lucius Malfoy\", \n",
    "                     \"Severus Snape\", \n",
    "                     \"Belatrix Lestrange\",\n",
    "                     \"Peter Pettigrew\"\n",
    "                ],\n",
    "    \"facial features\":  {\n",
    "                            \"nose\": null,\n",
    "                            \"skin\": \"ghastly\"\n",
    "                        }  \n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "json_dict = json.loads(json_text)\n",
    "pp(json_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7b4b2-0a39-485c-bcb8-e967974725c6",
   "metadata": {},
   "source": [
    "As we can see, the json.loads() function will parse a string containing JSON into a nested structure of python dictionaries and lists. To go in the other direction, we use the json.dumps() function. This direction requires an extra choice from the programmer: how do we want to format the text? There's not necessarily one right answer, just pick formatting parameters that make sense for you or your team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87cc89c7-5a99-4768-b020-57ce16bbf4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"birth year\": 1926,\n",
      "    \"evil?\": true,\n",
      "    \"facial features\": {\n",
      "        \"nose\": null,\n",
      "        \"skin\": \"ghastly\"\n",
      "    },\n",
      "    \"henchman\": [\n",
      "        \"Lucius Malfoy\",\n",
      "        \"Severus Snape\",\n",
      "        \"Belatrix Lestrange\",\n",
      "        \"Peter Pettigrew\"\n",
      "    ],\n",
      "    \"name\": \"Voldemort\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "new_text = json.dumps(json_dict, indent=4, sort_keys=True)\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9554fe2-9946-4c69-97b4-cbbaa935d01b",
   "metadata": {},
   "source": [
    "In case you have collected, processed, and formatted large quantities of data from an API, you should probably save that data for later use. This way, we don't have to bother the API for the same data more than once (very important), and you don't have to process it again. As an added bonus, this makes our data pipeline more **modular**, which helps us break the problem up into separate steps. We can write JSON to a file by using the json.dump() function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9c3bf4a-2c1e-40e8-9502-5278a859d9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./voldemort.json\", \"w\") as json_file:\n",
    "    json.dump(json_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4563b6-7a86-4618-a2de-57bfb6dec439",
   "metadata": {},
   "source": [
    "Check to see that \"voldemort.json\" has been added to the folder containing this file.\n",
    "\n",
    "When we're ready to come back and use our data, we can read the file using the json.load() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd8d609-167e-4f97-bfe5-f153cdd8c99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'birth year': 1926,\n",
      " 'evil?': True,\n",
      " 'facial features': {'nose': None, 'skin': 'ghastly'},\n",
      " 'henchman': ['Lucius Malfoy',\n",
      "              'Severus Snape',\n",
      "              'Belatrix Lestrange',\n",
      "              'Peter Pettigrew'],\n",
      " 'name': 'Voldemort'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"./voldemort.json\", \"r\") as json_file:\n",
    "    dict_from_file = json.load(json_file)\n",
    "    pp(dict_from_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b3aea3-dde6-4376-88f8-5c50c2322a75",
   "metadata": {},
   "source": [
    "WARNING: it is easy confuse the purpose of json.load() versus json.loads() and json.dump() versus json.dumps(). I find it helpful to pretend that the \"S\" in \"loads\" and \"dumps\" stands for \"String\", so these functions convert directly between **string** objects and dictionaries. By process of elimination, we can conclude that load and dump do not convert directly to strings, so they must interact with files instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8602e-ebea-4007-8a01-4d6f540db808",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8ee4a-138d-4279-91c1-4a29c0c87685",
   "metadata": {},
   "source": [
    "By now, we have gone over most of the tools for you to complete this problem on your own. If you get stuck, refer back to the section on basic python and pandas from cheatsheet1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83703155-0660-46f1-b855-2543f1cd953c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991c3c9-1bc8-4a6f-b3cf-be9c98723fe0",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f228a-6686-45ec-a520-3b41a982c136",
   "metadata": {},
   "source": [
    "Now we are going to get into the nitty gritty of machine learning. As you may know, machine learning is broadly used to describe algorithms that iteratively improve some model using data. As the model consumes more data, it tends to become more accurate, so we say that the machine \"learns\". This technique can take many different forms and create models for virtually any imaginable purpose. Roughly speaking, there are two coarse categories that we can separate machine learning algorithms into: supervised and unsupervised. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820154e5-57e2-485b-9158-35fc6fafcaef",
   "metadata": {},
   "source": [
    "**A supervised learning algorithm** learns by training on labeled data, which is comprised of many input-output pairs. At each iteration, the model takes an input and makes a prediciton on what the output should be. The learning algorithm then compares this predicted output with the actual output that was paired with the input in the label data set. Based on this comparison, the learning algorithm adjusts the model's internal parameters slightly in whatever direction would best improve the prediction to be closer to the actual output. By iterating over this process many times, the model will gradually have more and more accurate predictions. Thus supervised learning tends to be used to generate predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cb9de-64e0-40bf-a0c6-526aa8d7340e",
   "metadata": {},
   "source": [
    "**An unsupervised learning** learns by training on unlabeled data. Generally, these models attempt to determine some salient property of the dataset. The type of property we want to extract, and the algorithm we choose to extract it will largely depend on our use case. For example, if we want to discover naturally occuring clusters in our data, we might use the k-means algorithm. If we want to simplify a dataset with many variables, we might instead choose truncated singular value decomposition (SVD) to pick out features that best explain variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2520548a-8407-408c-8a18-dfbae003f047",
   "metadata": {},
   "source": [
    "### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba7c76-cc7e-4b18-8586-3e0d01a35b0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb546c0c-a498-40dd-ade4-9b28f52d8bf2",
   "metadata": {},
   "source": [
    "**Transformers** are a special type of artificial neural network that process sequential data such as text, images, or video. Like all neural networks, transformers are made up of layers of **neurons**, which are linked up by weighted connections. Also like all neural networks, transformers are primarily trained using a supervised learning algorithm called **backpropogation**, which iteratively tunes the weights of each connection starting from the output layer and working backwards to the input layer. In particular, transformers have an internal mechanism called self-attention that allow them to decide which parts of the data are most important, and give those parts of the data more weight in determining the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4036ede-3d80-4822-9389-82fb2768be6b",
   "metadata": {},
   "source": [
    "Transformers are often used to embed human-readable text into high dimensional vectors, which incredibly encode semantic information into geometric and algebraic relationships. For example, in the embedding given by the word2vec transformer, we might find that \"King\" - \"Man\" + \"Woman\" = \"Queen\", or that \"Actor\" - \"Talent\" + \"Ego\" = \"Jay Leto\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb4b6f-afda-4e95-9c2a-a0243ce0e5c5",
   "metadata": {},
   "source": [
    "![](embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a35ff-dcc8-4ccb-944f-524b39514c7c",
   "metadata": {},
   "source": [
    "In natural language processing, we will often use a transformer to map text to vectors as the first step in a larger model-training pipeline. In this exercise, you are tasked with using a pretrained transformer, SPECTER, to embed metadata text before using that embedding to train two other models. Let's try this on a dataset similar to the ones you generated in exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c1111e8-3f3b-4b9b-9747-7cb0fc171e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stroke.json') as file:\n",
    "    stroke_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf80e9-ce66-4ec2-892d-6207ff426dcf",
   "metadata": {},
   "source": [
    "Now that we have some data, let's download and import the tools we will need to parse and embed our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "232a310d-12a7-4113-b4b8-9d042e466eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: | \n",
      "Warning: 2 possible package resolutions (only showing differing packages):\n",
      "  - anaconda/linux-64::certifi-2021.10.8-py39h06a4308_0\n",
      "  - defaults/linux-64::certifi-2021.10.8-py39h06a4308done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#download transformers using conda in terminal\n",
    "!conda install -c huggingface transformers -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f553d701-7be8-420f-a7b0-d633fc50d9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import math\n",
    "\n",
    "# load tokenizer, which parses texts into tokens, which can be fed into the model\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "\n",
    "#load model, which is used to embed our text data as vectors\n",
    "model = AutoModel.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a324ad-524b-4b36-a420-762241676727",
   "metadata": {},
   "source": [
    "Now that we have our tools imported, let's use them to create our embeddings. The result of the following cell, embeddings, is a numpy array where the i'th element of the array is a 768-dimensional vector that embeds the i'th paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d45b929b-de0d-4eb3-b831-84fcec4df696",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2170144/3170900250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep_token\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpaper\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"abstract\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpapers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# take the first token in the batch as the embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2401\u001b[0m                 )\n\u001b[1;32m   2402\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   2404\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2586\u001b[0m         )\n\u001b[1;32m   2587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2588\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   2589\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2590\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         encodings = self._tokenizer.encode_batch(\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "papers = list(stroke_dict.values())\n",
    "batch_size = 100\n",
    "batch_num = math.floor(len(papers)/batch_size)\n",
    "\n",
    "\n",
    "for i in range(batch_num):\n",
    "    data.append( [paper[\"title\"] + tokenizer.sep_token + paper[\"abstract\"] for paper in papers[batch_size*i : batch_size*(i+1)]])\n",
    "    \n",
    "inputs = tokenizer(data, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "result = model(**inputs)\n",
    "# take the first token in the batch as the embedding\n",
    "embeddings = result.last_hidden_state[:, 0, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656c31b-a342-497a-b819-995be19f50b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485259a3-6abe-4cf4-97f5-780a2281667b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a73f7c-c33e-44d2-9cf5-3971415d3ca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd9f1276-6ff4-4326-b4e7-edfda0fbd4ba",
   "metadata": {},
   "source": [
    "## PCA and LCA:\n",
    "\n",
    "Both Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are linear methods used for dimensionality reduction, but they differ in their objective. PCA seeks to build a small-dimensional coordinate system using whatever small collection of components (i.e. combinations of variables) best explains the variability of the underlying dataset, and then simply throw out the coordinates do not have a high impact on variation. LDA on the other hand seeks to project the dataset onto a lower dimensional space in a way that best preserves separation of distinct data classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ea777-4388-4815-bab1-d3515f985957",
   "metadata": {},
   "source": [
    "![PCA vs LDA](PCA_vs_LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab8df4-850d-481e-b24d-2d500aa75d63",
   "metadata": {},
   "source": [
    "[image source](https://medium.com/analytics-vidhya/pca-vs-lda-vs-t-sne-lets-understand-the-difference-between-them-22fa6b9be9d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc359ef6-e2d4-48ae-a001-f9a700ba1f63",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805069f9-442e-4e56-a0ac-7df94929457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "embeddings_pca = pd.DataFrame(\n",
    "    pca.fit_transform(embeddings),\n",
    "    columns=['PC0', 'PC1', 'PC2']\n",
    ")\n",
    "embeddings_pca[\"query\"] = [paper[\"query\"] for paper in papers.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43276737-ffa4-45e8-9af5-9509e8fcc273",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd56315-3950-45c6-907a-4083ee73f697",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d54467-e79c-407b-afbe-a4fbb7ee8394",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf10e86-23bd-4744-b224-b654b2bdc61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "embeddings_lda = pd.DataFrame(\n",
    "    lda.fit_transform(embeddings, categories), columns=[\"lda0\"]\n",
    ")\n",
    "\n",
    "embeddings_lda[\"query\"] = categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13046d33-6691-4cae-a710-a072f1729fdf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfaa834-3e46-4a59-a1c3-6cae9d1ce377",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedb264-dfa3-4933-be0e-13c18d2ec523",
   "metadata": {},
   "source": [
    "Here you are tasked with describing how to parallelize the merge sort algorithm using two processes. Before you try to parallelize it, let's try to get a good understanding of how the merge sort works in general. Suppose you would like to sort the following list of values into assending order:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c1dcd-0365-4886-ab81-8b397dc77349",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7, 6, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f2cf4-1da9-4410-b39a-08d50afac98b",
   "metadata": {},
   "source": [
    "We start by splitting the list into two equal halfs, effectively by making a copy of each half:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a03f79-2d48-41e7-8614-5279cd8de8db",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7, 6, 1]\n",
    "```\n",
    "$\\hspace{3.5em}\\swarrow\\searrow$\n",
    "```json\n",
    "[[5,7],[6,1]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a165db1-1e34-4741-8d8b-f802bb41e58c",
   "metadata": {},
   "source": [
    "We continue to recursively break up each sub-list one at a time until we encounter two lists containing at most one element each. Breaking up the first sublist above, we arrive at the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bc26c-bf02-4bb2-8ed9-b84ff520b2e2",
   "metadata": {},
   "source": [
    "```json\n",
    "[5, 7]\n",
    "```\n",
    "\n",
    "$\\hspace{2em}\\swarrow\\searrow$\n",
    "\n",
    "```json\n",
    "[[5],[7]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724a31a2-b45d-43c3-a93f-8c3184f3b989",
   "metadata": {},
   "source": [
    "Now that we have two sublists of size 1, it is time to merge. We accomplish this by comparing the first element from each sublist. Whichever is smaller goes is removed from the split sublist and appended to the end of the sublist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde7452-c7c6-4d09-a470-d70464818ccc",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5],[7]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 5 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[7]],\n",
    "    merged: [5]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288d52dd-d09b-4a49-a491-d33465533b6c",
   "metadata": {},
   "source": [
    "Now we try to compare the first element of each of the split sublists. Since the first sublist has run out of elements, we have to put the rest of them in the merged sublist. We've already compared the smallest element of the second sublist with all of the elements of the merged list, so we can conclude that the the second sublist can get added to the end of the merged list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7313a3f-5a6b-4860-95ee-48e1b3e823ad",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[],[7]],\n",
    "    merged: [5]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of our second sublist are appended to the merged list\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [5,7]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2cecd-931f-44ab-ad43-3b46410a8efd",
   "metadata": {},
   "source": [
    "Now that the first pair of single-element sublists are merged, we rinse and repeat with the next sublist:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37035dc-5944-47c8-a2c1-c08cc63a970d",
   "metadata": {},
   "source": [
    "### Split\n",
    "```json\n",
    "[6, 1]\n",
    "```\n",
    "\n",
    "$\\hspace{2em}\\swarrow\\searrow$\n",
    "\n",
    "```json\n",
    "[[6],[1]]\n",
    "```\n",
    "\n",
    "### Merge\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[6],[1]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 1 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[6],[]],\n",
    "    merged: [1]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of our first sublist are appended to the merged list\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [1,6]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33f604-0032-4653-96f9-7cb3327b5f7b",
   "metadata": {},
   "source": [
    "Now all of the single element sublists have been merged into sorted lists. We start merging these larger lists, which is only slightly more complicated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c015283-6407-4483-9b31-d645dde8366d",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[1,6]],\n",
    "    merged: []\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8be5f5-202e-4a83-b7f7-4c79e6d73aa0",
   "metadata": {},
   "source": [
    "We compare the first element of each split sublist. Whichever is smaller goes in the merged sublist first:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5e1f7b-ec89-4338-a09b-e934bf88213c",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[1,6]],\n",
    "    merged: []\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 1 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[5,7],[6]],\n",
    "    merged: [1]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1761806-9509-4bcb-a3f9-0ca1362c21e7",
   "metadata": {},
   "source": [
    "Now the second split sublist has a new smallest element, so we have to make the comparison again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68291c-d586-4374-bee9-e2df708c2381",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[5,7],[6]],\n",
    "    merged: [1]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ 5 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[7],[6]],\n",
    "    merged: [1,5]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a11012-0bcd-44aa-a271-c244e4cc2a57",
   "metadata": {},
   "source": [
    "Repeating this step one more time, we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312ca3b-cb67-4d28-b367-d92ac91ba786",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[7],[6]],\n",
    "    merged: [1,5]\n",
    "}\n",
    "```\n",
    "$\\hspace{4em}\\Downarrow$ 6 is removed from split and added to merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[7],[]],\n",
    "    merged: [1,5,6]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a134d-e7a6-4cb2-b852-4f3206116bdc",
   "metadata": {},
   "source": [
    "Again, one of our split sublists has run out of elements, so we have to merge the rest of the other sublist into the merged list. Since the smallest element of this split sublist has already been compared with the largest element of the merged sublist, every element of the split sublist is larger than every element of the merged sublist. Hence, we just append all the elements from the split sublist to the end of the merged sublist:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b769d2-48bd-40f4-91bf-54e72881016e",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    split: [[7],[]],\n",
    "    merged: [1,5,6]\n",
    "}\n",
    "```\n",
    "\n",
    "$\\hspace{4em}\\Downarrow$ remaining elements of split sublist appended to the end of merged\n",
    "\n",
    "```json\n",
    "{\n",
    "    split: [[],[]],\n",
    "    merged: [1,5,6,7]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908f70b-ce45-4653-b530-6fdefadeb6d2",
   "metadata": {},
   "source": [
    "And so our list is sorted, and our task is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4615ca-5c7e-4c2a-86db-6263567e9b4b",
   "metadata": {},
   "source": [
    "### Disclaimer:\n",
    "The explanation above is for illustrative purposes only. Making many copies of lists and resizing lists both use lots of memory and can slow down your program as your dataset scales. It is more efficient to define a by specifying its beginning and ending indices, and to \"remove\" an element from a sublist by changing those index bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b74f25-659a-4c56-8267-6c0c1cd785c3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201b12f9-36a6-445b-8f73-4a0d53ec6112",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79247bd4-3aef-4456-aa6b-e6b69df5dab2",
   "metadata": {},
   "source": [
    "You should already have most of the tools you need to answer this question, however we shall briefly review missing data, as well as some basic strategies for dealing with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9e5fed-838d-4a62-bf86-b526c8e90853",
   "metadata": {},
   "source": [
    "### There are three main forms that missing data can take ordered from best case to worst case scenario:\n",
    "\n",
    " 1. Missing Completely at Random (MCAR): there is no reason for missing data inherent to the data itself. In other words, all entries are equally likely to be missing.\n",
    " 2. Missing at Random (MAR): whether or not an entry is missing correlates to one or more known variables.\n",
    " 3. Missing Not at Random (MNAR): whether or not an entry is missing correlates to some unknown variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792de93c-5536-4bbd-8b36-b8d38e4a0197",
   "metadata": {},
   "source": [
    "### Quick and Dirty MAR/MNAR Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad4628-6d31-49cb-aa9c-9b85551934dd",
   "metadata": {},
   "source": [
    "To detect MAR/MNAR data, choose a field with some missing entries. According to this field, we split our dataset into two groups:\n",
    "\n",
    "1. A group of all rows missing that field's entry\n",
    "2. A group of all rows not missing that field's entry\n",
    "\n",
    "Check to see if the summary statistics of each group are similar. If those summary statistics differ by a significant amount, it's a sign that our data is either MAR or MNAR. If the summary statistics of the two groups are very similar, it doesn't prove anything, but it at leasts suggests that your data might be MCAR.\n",
    "\n",
    "**Note**: In practice, it is often impossible to tell the difference between MNAR and MAR since the data you would need to determine this difference is precisely the data that is missing.\n",
    "\n",
    "Remember you may multiple fields with missing data, so it's a good idea to run the above process for every field with missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626686d9-ade4-4071-9b6b-c3f1074822de",
   "metadata": {},
   "source": [
    "### Little's Test for MCAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8789e47-abf3-42ea-8ac9-103b1976006b",
   "metadata": {},
   "source": [
    "Little's test is more rigorous than the above quick test, but more involved. The internal workings of this test are beyond the scope of this class, but the short explanation is that technique defines a test statistic that approaches $\\chi^2$ (chi-squared) for large datasets. There are available packages in R and SPSS to run this test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b43994-b897-4bc7-bf2a-4c43f0486313",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1084f4-870e-4512-bc80-1102db34af84",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cabb3b-8030-4952-81f4-e3e4275e1812",
   "metadata": {},
   "source": [
    "There are four basic strategies we can use to deal with missing data:\n",
    "1. **Ignore it:** pretend that the problem isn't real and can't hurt you.\n",
    "2. **Drop rows:** drop any row that is missing entries in a field that we care about. \n",
    "3. **Drop the field:** drop any field that is missing too many entries to salvage.\n",
    "4. **Impute missing values:** make an educated guess at what the missing value **could** be based on the data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb17775-50e2-44ed-9e7f-24452d0caa1f",
   "metadata": {},
   "source": [
    "### Ignoring Missing Data\n",
    "\n",
    "Obviously this is risky, and often inadvisable. There is little more to say here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c38c6-245e-4ef8-8321-a2454ef16530",
   "metadata": {},
   "source": [
    "### Dropping Rows\n",
    "\n",
    "#### Listwise vs Pairwise Deletion\n",
    "There are two ways we can drop rows:\n",
    "\n",
    "1. **Listwise deletion:** Before any analyses begin, delete all the rows with values missing in any of the fields we will use for *any* of our analyses.\n",
    "2. **Pairwise deletion:** Before an individual analysis, delete only the rows with entries missing in the fields we will use for *that particular* analysis.\n",
    "\n",
    "Listwise deletion is a bit easier, but often results in us deleting more data than we have to.\n",
    "Pairwise deletion is a bit harder, but usually preserves more data for each individual analysis.\n",
    "\n",
    "For example, suppose we wanted to run two separate linear regressions: one regression between variables A & B, and one regression between A & C. If we used listwise deletion, we would start by deleting all the rows missing any entries in fields A, B, or C. Only then would we run our regressions. Notice that this means our A-B regression might then lose out on some rows that are only missing the C entry, which is totally irrelevant to that analysis.\n",
    "\n",
    "Supposing we wanted to run the same regressions, but instead chose to use pairwise deletion. We would start by removing all the rows with missing data in the A or B fields from a copy of our original dataset, then run our A-B regression on that filtered dataset. Then we would remove all the rows with missing data in the A or C fields from another copy of our original dataset, and run the A-C regression on the new filtered dataset. By doing so, our A-B regression won't lose out on any rows due to any missing values in the C column.\n",
    "\n",
    "#### Risks of Dropping Rows\n",
    "\n",
    "Since dropping rows decreases the size of the sample you can use for analyses, this process decreases the statistical power of your models. This is doubly relevant for listwise deletion, since that method usually results in more dropped rows.\n",
    "\n",
    "This method will also bias your data if it is not MCAR, since some properties of your data will correlate with missing entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5b3c1e-bb89-4035-aa30-e1099f94c0c7",
   "metadata": {},
   "source": [
    "### Dropping Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fab8f3f-9bfd-4ed0-af1e-bc69cf481a69",
   "metadata": {},
   "source": [
    "If a given field has a high proportion of missing data, analyses based on that field may not be very accurate, and are unlikely to generalize well. If this is the case, you should probably drop that field entirely.\n",
    "\n",
    "This will not bias your data like dropping rows can, but having fewer variables may reduce the power of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be4b8c-0e7d-4f1c-83bb-8871f0966529",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17417ef2-14b0-4716-83f5-bf78f6e38a5b",
   "metadata": {},
   "source": [
    "Imputation is a technique where we predict what a missing value should be based on the rest of the dataset. Imputation has a distinct advantage over dropping strategies: it yields more data. This can improve the power of our analyses **if we chose our imputed values wisely**. Chosing our imputed values unwisely can result in totally worthless models, so be careful when you're cleaning your data this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda55561-5d8a-40ff-9003-70663c6b86b9",
   "metadata": {},
   "source": [
    "#### Bad Imputation Strategies (do not attempt at home):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30098c4e-672f-4f78-85f8-cede01e99e81",
   "metadata": {},
   "source": [
    "Here are a tips if you would like to stay in the good graces of your graders and employers:\n",
    "\n",
    "1. **DO NOT** use the mean, median, mode, etc. of a field to fill that field's missing values\n",
    "    - e.g. $\\textit{missing height} = \\mathbb{E}[\\textit{height}] $\n",
    "2. **DO NOT** use linear regression predictions to replace missing values\n",
    "    - e.g. $\\textit{missing value} = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_n x_n$\n",
    "3. **DO NOT** use stochastic regression predictions either\n",
    "    - e.g. $\\textit{missing value} = \\beta_0 + \\beta_1 x_1 + \\ldots + \\beta_n x_n + \\varepsilon_i$, where $\\varepsilon_i \\sim N(0,s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9078a685-7689-42a6-aeaa-cbac1faf88a9",
   "metadata": {},
   "source": [
    "#### Deterministic Imputation (attempt at home)\n",
    "\n",
    "If there is some logical reason why some values in a given row imply a missing entry can only have one value, then this is our best option.\n",
    "For example, if you already know a company's income and expenses for the quarter, you can fill its missing profit entry for that quarter with income - expenses. Or if you know that a patient is younger than 65, then by definition, that patient cannot have late-onset dementia. \n",
    "\n",
    "The only risk here is that your logical reasoning is incorrect. As long as the rules you use to choose your imputed values are sound, then this method can do no wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e8ff5-37f8-44c6-bcd0-e1792bf1dee4",
   "metadata": {},
   "source": [
    "#### Multiple Imputation (attempt at home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d4262a-d91c-41ef-94d2-9fa48e6b633d",
   "metadata": {},
   "source": [
    "Multiple imputation is simply the process of making many predictions for a missing value and then combining those predictions to get some final imputed value. This technique can vary in the way we choose to make our predictions, as well as the way we choose to combine them. One valid method is to use the non-missing portions of our data to construct conditional distributions for each of the missing values given other values in that row. Drawing from these conditional distributions many times, using those drawn values to construct predictors, and then combining those predictors with averaging or voting will yield our final predictor. Two popular methods for using values drawn from conditional distributions to construct predictors are the Full Information Maximum Likelihood (FIML) and Expectation Maximization (EM) methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
